{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\vscode\\DL_practice\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import interleave_datasets, load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "from datasets import load_from_disk\n",
    "from pprint import pprint\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset('stanfordnlp/imdb')\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_train_split = imdb_dataset['train']\n",
    "print(imdb_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "_ = imdb_dataset.pop('unsupervised')\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# to download only the train split \n",
    "train_split = load_dataset('stanfordnlp/imdb', split='train')\n",
    "print(train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can train on one specific train split and test on a diff test split of a diff dataset \n",
    "\n",
    "\n",
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "small_ds = train_split.train_test_split(test_size=0.2)\n",
    "print(small_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# import local files \n",
    "# it has both train and test data \n",
    "data_files =['data/train.csv', 'data/test.csv']\n",
    "local_dataset = load_dataset('csv', data_files=data_files)\n",
    "print(local_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = local_dataset[\"train\"].train_test_split(test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if its a larger dataset then convert it into `pyarrow(.arrow)` and then save it to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5/5 [00:00<00:00, 833.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6/6 [00:00<00:00, 2966.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_split.save_to_disk('pyarrow_dataset/movie_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_dataset_from_disk = load_from_disk('pyarrow_dataset/movie_review')\n",
    "print(raw_dataset_from_disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'Although I have to admit I laughed more watching this movie than the '\n",
      "         'last few comedies I saw.<br /><br />The budget must have consisted '\n",
      "         'of pocket change from the actors. The production values are so low '\n",
      "         'that they actual made it kind of fun to watch. Reminds me of the '\n",
      "         'Robot Monster made up of a guy in a gorilla suit with a cardboard '\n",
      "         'diving helmet on.<br /><br />In one scene a hapless victim gets '\n",
      "         'their arm and leg cut off. Geez, hard to believe but the Black '\n",
      "         'Knight scene from Holy Grail was more realistic. I kept wondering '\n",
      "         'why the victim didn\\'t start shouting \" None Shall Pass\" and \" It\\'s '\n",
      "         'only a flesh wound, I\\'ve had worse\". It was one of the funniest '\n",
      "         'scenes I\\'ve seen in the past year.<br /><br />The \"gladiator/demon\" '\n",
      "         'was a stitch too. Between the horribly cheap costume and the geeky '\n",
      "         'look of the guy in it the end result was hysterical.<br /><br '\n",
      "         '/>Truly a movie that is bad enough to be watchable. Kind of like '\n",
      "         'seeing a slow motion auto accident on film.<br /><br />'}\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "idx = 1000\n",
    "example = imdb_dataset['train'][idx]\n",
    "pprint(example)\n",
    "\n",
    "subset = imdb_dataset['train'].select([0, 1])\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use these subsets for testing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "idx = range(0, 100, 2)\n",
    "subset = imdb_dataset['train'].select(idx)\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMT14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs-en', 'de-en', 'fr-en', 'hi-en', 'ru-en']\n",
      "['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "print(get_dataset_config_names('wmt/wmt14'))\n",
    "print(get_dataset_split_names('wmt/wmt14', 'hi-en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 32863\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 520\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2507\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "translation_dataset = load_dataset(path = 'wmt/wmt14', name='hi-en')\n",
    "print(translation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 35890\n",
      "})\n",
      "35890\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(path = 'wmt/wmt14', name='hi-en', split='train+test+validation')\n",
    "print(raw_dataset)\n",
    "print(len(raw_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': Translation(languages=['hi', 'en'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(translation_dataset['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "mrpc_dataset = load_dataset('glue', 'mrpc', split='train')\n",
    "print(mrpc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': Value(dtype='int32', id=None),\n",
      " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
      " 'sentence1': Value(dtype='string', id=None),\n",
      " 'sentence2': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(mrpc_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering\n",
      "--------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print('before filtering')\n",
    "print(20*'-')\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the samples have been split across the 16 cpus and the condition has been checked "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 77647.33 examples/s]\n",
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 81921.22 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filtering\n",
      "--------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 22074\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 21909\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_words = 100 \n",
    "imdb_filtered_dataset = imdb_dataset.filter(\n",
    "    lambda example: len(example['text'].split(' ')) >= num_words\n",
    ")\n",
    "print('after filtering')\n",
    "print(20*'-')\n",
    "print(imdb_filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "('IMDB:Terrible movie. Nuff Said.<br /><br />These Lines are Just Filler. The '\n",
      " \"movie was bad. Why I have to expand on that I don't know. This is already a \"\n",
      " 'waste of my time. I just wanted to warn others. Avoid this movie. The acting '\n",
      " 'sucks and the writing is just moronic. Bad in every way. The only nice thing '\n",
      " \"about the movie are Deniz Akkaya's breasts. Even that was ruined though by a \"\n",
      " 'terrible and unneeded rape scene. The movie is a poorly contrived and '\n",
      " 'totally unbelievable piece of garbage.<br /><br />OK now I am just going to '\n",
      " 'rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste '\n",
      " 'my time watching this offal. Then feeling compelled to warn others I create '\n",
      " 'an account with IMDb only to discover that I have to write a friggen essay '\n",
      " 'on the film just to express how bad I think it is. Totally unnecessary.')\n"
     ]
    }
   ],
   "source": [
    "def add_prefix(example) : \n",
    "    example['text'] = \"IMDB:\"+example[\"text\"]\n",
    "    return example\n",
    "\n",
    "imdb_prefixed_dataset = imdb_dataset.map(add_prefix)\n",
    "print(imdb_prefixed_dataset)\n",
    "pprint(imdb_prefixed_dataset['train']['text'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**concatinate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset_whole = load_dataset('stanfordnlp/imdb', split='train+test')\n",
    "print(imdb_dataset_whole)\n",
    "print(imdb_dataset_whole.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10662\n",
      "})\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "rt_dataset_whole = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='all')\n",
    "print(rt_dataset_whole)\n",
    "print(rt_dataset_whole.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 60662\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "concat_dataset = datasets.concatenate_datasets([imdb_dataset_whole, rt_dataset_whole], axis=0)\n",
    "print(concat_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**interleaving datasets**\n",
    "\n",
    "it will select 60% from imdb and 40%(all) from rotten_tomato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 26577\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "inter_datasets = interleave_datasets(\n",
    "    [imdb_dataset_whole, rt_dataset_whole], probabilities=[0.6, 0.4]\n",
    ")\n",
    "print(inter_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iterable dataset**\n",
    "\n",
    "stream one sample at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_shards: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_itr_dataset = load_dataset('stanfordnlp/imdb', split='train', streaming=True)\n",
    "print(imdb_itr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the '\n",
      "         'controversy that surrounded it when it was first released in 1967. I '\n",
      "         'also heard that at first it was seized by U.S. customs if it ever '\n",
      "         'tried to enter this country, therefore being a fan of films '\n",
      "         'considered \"controversial\" I really had to see this for myself.<br '\n",
      "         '/><br />The plot is centered around a young Swedish drama student '\n",
      "         'named Lena who wants to learn everything she can about life. In '\n",
      "         'particular she wants to focus her attentions to making some sort of '\n",
      "         'documentary on what the average Swede thought about certain '\n",
      "         'political issues such as the Vietnam War and race issues in the '\n",
      "         'United States. In between asking politicians and ordinary denizens '\n",
      "         'of Stockholm about their opinions on politics, she has sex with her '\n",
      "         'drama teacher, classmates, and married men.<br /><br />What kills me '\n",
      "         'about I AM CURIOUS-YELLOW is that 40 years ago, this was considered '\n",
      "         'pornographic. Really, the sex and nudity scenes are few and far '\n",
      "         \"between, even then it's not shot like some cheaply made porno. While \"\n",
      "         'my countrymen mind find it shocking, in reality sex and nudity are a '\n",
      "         'major staple in Swedish cinema. Even Ingmar Bergman, arguably their '\n",
      "         'answer to good old boy John Ford, had sex scenes in his films.<br '\n",
      "         '/><br />I do commend the filmmakers for the fact that any sex shown '\n",
      "         'in the film is shown for artistic purposes rather than just to shock '\n",
      "         'people and make money to be shown in pornographic theaters in '\n",
      "         'America. I AM CURIOUS-YELLOW is a good film for anyone wanting to '\n",
      "         'study the meat and potatoes (no pun intended) of Swedish cinema. But '\n",
      "         \"really, this film doesn't have much of a plot.\"}\n",
      "{'label': 0,\n",
      " 'text': '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. '\n",
      "         \"It doesn't matter what one's political views are because this film \"\n",
      "         'can hardly be taken seriously on any level. As for the claim that '\n",
      "         \"frontal male nudity is an automatic NC-17, that isn't true. I've \"\n",
      "         'seen R-rated films with male nudity. Granted, they only offer some '\n",
      "         'fleeting views, but where are the R-rated films with gaping vulvas '\n",
      "         \"and flapping labia? Nowhere, because they don't exist. The same goes \"\n",
      "         'for those crappy cable shows: schlongs swinging in the breeze but '\n",
      "         'not a clitoris in sight. And those pretentious indie movies like The '\n",
      "         \"Brown Bunny, in which we're treated to the site of Vincent Gallo's \"\n",
      "         'throbbing johnson, but not a trace of pink visible on Chloe Sevigny. '\n",
      "         'Before crying (or implying) \"double-standard\" in matters of nudity, '\n",
      "         'the mentally obtuse should take into account one unavoidably obvious '\n",
      "         'anatomical difference between men and women: there are no genitals '\n",
      "         'on display when actresses appears nude, and the same cannot be said '\n",
      "         \"for a man. In fact, you generally won't see female genitals in an \"\n",
      "         'American film in anything short of porn or explicit erotica. This '\n",
      "         'alleged double-standard is less a double standard than an admittedly '\n",
      "         'depressing ability to come to terms culturally with the insides of '\n",
      "         \"women's bodies.\"}\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for example in imdb_itr_dataset:\n",
    "    pprint(example)\n",
    "    counter += 1 \n",
    "    if counter == 2 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'IMDB:I rented I AM CURIOUS-YELLOW from my video store because of all '\n",
      "         'the controversy that surrounded it when it was first released in '\n",
      "         '1967. I also heard that at first it was seized by U.S. customs if it '\n",
      "         'ever tried to enter this country, therefore being a fan of films '\n",
      "         'considered \"controversial\" I really had to see this for myself.<br '\n",
      "         '/><br />The plot is centered around a young Swedish drama student '\n",
      "         'named Lena who wants to learn everything she can about life. In '\n",
      "         'particular she wants to focus her attentions to making some sort of '\n",
      "         'documentary on what the average Swede thought about certain '\n",
      "         'political issues such as the Vietnam War and race issues in the '\n",
      "         'United States. In between asking politicians and ordinary denizens '\n",
      "         'of Stockholm about their opinions on politics, she has sex with her '\n",
      "         'drama teacher, classmates, and married men.<br /><br />What kills me '\n",
      "         'about I AM CURIOUS-YELLOW is that 40 years ago, this was considered '\n",
      "         'pornographic. Really, the sex and nudity scenes are few and far '\n",
      "         \"between, even then it's not shot like some cheaply made porno. While \"\n",
      "         'my countrymen mind find it shocking, in reality sex and nudity are a '\n",
      "         'major staple in Swedish cinema. Even Ingmar Bergman, arguably their '\n",
      "         'answer to good old boy John Ford, had sex scenes in his films.<br '\n",
      "         '/><br />I do commend the filmmakers for the fact that any sex shown '\n",
      "         'in the film is shown for artistic purposes rather than just to shock '\n",
      "         'people and make money to be shown in pornographic theaters in '\n",
      "         'America. I AM CURIOUS-YELLOW is a good film for anyone wanting to '\n",
      "         'study the meat and potatoes (no pun intended) of Swedish cinema. But '\n",
      "         \"really, this film doesn't have much of a plot.\"}\n"
     ]
    }
   ],
   "source": [
    "for example in imdb_itr_dataset.map(add_prefix):\n",
    "    pprint(example) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']\n"
     ]
    }
   ],
   "source": [
    "print(len(get_dataset_config_names('ai4bharat/naamapadam')))\n",
    "print(get_dataset_config_names('ai4bharat/naamapadam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 985787\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 867\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 13460\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset('ai4bharat/naamapadam', 'hi')\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 497882\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 758\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 2795\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('ai4bharat/naamapadam', 'ta')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [{'filename': 'C:\\\\Users\\\\ghora\\\\.cache\\\\huggingface\\\\datasets\\\\ai4bharat___naamapadam\\\\ta\\\\1.0.0\\\\9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\\\\naamapadam-train.arrow'}],\n",
       " 'test': [{'filename': 'C:\\\\Users\\\\ghora\\\\.cache\\\\huggingface\\\\datasets\\\\ai4bharat___naamapadam\\\\ta\\\\1.0.0\\\\9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\\\\naamapadam-test.arrow'}],\n",
       " 'validation': [{'filename': 'C:\\\\Users\\\\ghora\\\\.cache\\\\huggingface\\\\datasets\\\\ai4bharat___naamapadam\\\\ta\\\\1.0.0\\\\9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\\\\naamapadam-validation.arrow'}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.cache_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.28880310058594\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "train_file = ds.cache_files['train'][0]['filename']\n",
    "test_file = ds.cache_files['test'][0]['filename']\n",
    "validation_file = ds.cache_files['validation'][0]['filename']\n",
    "\n",
    "total_size = sum(\n",
    "    os.path.getsize(file) for file in [train_file, test_file, validation_file]\n",
    ")\n",
    "\n",
    "print(total_size/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['பைரவருக்கு',\n",
       "  'தேய்பிறை',\n",
       "  'அஷ்டமியில்',\n",
       "  'விசேஷ',\n",
       "  'அபிஷேக',\n",
       "  'ஆராதனைகள்',\n",
       "  'நடைபெறுகின்றன',\n",
       "  '.'],\n",
       " 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6001876\n"
     ]
    }
   ],
   "source": [
    "def compute_num_tokens(example) :\n",
    "    return {\"num_tokens\" : len(example['tokens'])}\n",
    "\n",
    "ds = ds.map(compute_num_tokens)\n",
    "\n",
    "total_tokens = sum(ds['train']['num_tokens']) + sum(ds['test']['num_tokens']) + sum(ds['validation']['num_tokens'])\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['பைரவருக்கு',\n",
       "  'தேய்பிறை',\n",
       "  'அஷ்டமியில்',\n",
       "  'விசேஷ',\n",
       "  'அபிஷேக',\n",
       "  'ஆராதனைகள்',\n",
       "  'நடைபெறுகின்றன',\n",
       "  '.'],\n",
       " 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'num_tokens': 8}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183.1378173828125\n"
     ]
    }
   ],
   "source": [
    "train_file = ds.cache_files['train'][0]['filename']\n",
    "test_file = ds.cache_files['test'][0]['filename']\n",
    "validation_file = ds.cache_files['validation'][0]['filename']\n",
    "\n",
    "total_size = sum(\n",
    "    os.path.getsize(file) for file in [train_file, test_file, validation_file]\n",
    ")\n",
    "\n",
    "print(total_size/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 501435/501435 [00:21<00:00, 23412.90 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['num_tokens', 'text'],\n",
       "    num_rows: 501435\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_combined = datasets.concatenate_datasets([ds['train'], ds['test'], ds['validation']])\n",
    "\n",
    "def combine_tokens(example):\n",
    "    return {\"text\": \" \".join(example[\"tokens\"])}\n",
    "\n",
    "ds_combined = ds_combined.map(combine_tokens)\n",
    "ds = ds_combined.remove_columns([\"ner_tags\", \"tokens\"])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 501435/501435 [00:01<00:00, 390749.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filtering\n",
      "--------------------\n",
      "Dataset({\n",
      "    features: ['num_tokens', 'text'],\n",
      "    num_rows: 370495\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_filtered_dataset = ds.filter(\n",
    "    lambda example: example['num_tokens'] >= 6\n",
    ")\n",
    "print('after filtering')\n",
    "print(20*'-')\n",
    "print(ds_filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5346\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "glue_ds = load_dataset('ai4bharat/indic_glue', 'inltkh.ta')\n",
    "print(glue_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'கே.வி.ஆனந்தே ட்விட்டரில் இதை அறிவித்துள்ளார். இந்தப் படத்துக்கு கேவ்மிக் ஆரி ஒளிப்பதிவு செய்ய, ஹாரிஸ் ஜெயராஜ் இசையமைக்கிறார். பட்டுக்கோட்டை பிரபாகர் வசனம் எழுத, கலை இயக்குநராக கிரண் பணியாற்றுகிறார். இந்தப் படத்தை லைகா புரொடக்\\u200cஷன்ஸ் நிறுவனம் தயாரிக்கிறது.',\n",
       " 'label': 6}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5346/5346 [00:00<00:00, 102810.90 examples/s]\n",
      "Filter: 100%|██████████| 669/669 [00:00<00:00, 26762.19 examples/s]\n",
      "Filter: 100%|██████████| 669/669 [00:00<00:00, 24890.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "glue_ds_filter = glue_ds.filter(\n",
    "    lambda example: len(example['text'].split(' ')) >= 6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 6428\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_ds_filter_combined = datasets.concatenate_datasets([glue_ds_filter['train'], glue_ds_filter['test'], glue_ds_filter['validation']])\n",
    "glue_ds_filter_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['num_tokens', 'text', 'label'],\n",
      "    num_rows: 32354\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "inter_datasets = interleave_datasets(\n",
    "    [ds_filtered_dataset, glue_ds_filter_combined], probabilities=[0.8, 0.2], seed=42\n",
    ")\n",
    "print(inter_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
