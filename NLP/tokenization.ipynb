{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint \n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE # Byte-Pair Encoding\n",
    "from tokenizers import Tokenizer\n",
    "from ipywidgets import interact, IntSlider, Layout\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.tools import EncodingVisualizer\n",
    "from multiprocessing import cpu_count\n",
    "import pandas as pd \n",
    "\n",
    "print(cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline** \n",
    "\n",
    "normalizer (lowercase) -> pre-tokenizer (whitespace) -> model (BPE) -> postprocessor (None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 74004228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('bookcorpus', split='train', trust_remote_code=True)\n",
    "pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : usually , he would be tearing around the living room , playing with his toys .\n",
      "1 : but just one look at a minion sent him practically catatonic .\n",
      "2 : that had been megan 's plan when she got him dressed earlier .\n",
      "3 : he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\n",
      "4 : she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .\n",
      "5 : `` are n't you being a good boy ? ''\n"
     ]
    }
   ],
   "source": [
    "num_samples = 6 \n",
    "for idx, sample in enumerate(ds[0:num_samples]['text']):\n",
    "    print(f'{idx} : {sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPE(ukn_token='[UNK]') # for unknown tokens output UNK\n",
    "tokenizer = Tokenizer(model=model)\n",
    "\n",
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size = 32000, special_tokens = ['[UNK]', '[PAD]'], \n",
    "    continuing_subword_prefix='##'\n",
    ") #runnning --> run ##ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(batch_size=1000) : \n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i:i+batch_size]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    get_examples(batch_size=10000), trainer=trainer, \n",
    "    length=len(ds)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/vscode/DL_practice/NLP/models\\\\hopper-vocab.json',\n",
       " 'D:/vscode/DL_practice/NLP/models\\\\hopper-merges.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('D:/vscode/DL_practice/NLP/models', prefix='hopper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#version: 0.2\n",
      "##h ##e\n",
      "t ##he\n",
      "##i ##n\n",
      "##e ##r\n",
      "##e ##d\n",
      "##o ##u\n",
      "##n ##d\n",
      "##in ##g\n",
      "t ##o\n"
     ]
    }
   ],
   "source": [
    "with open('models/hopper-merges.txt', 'r') as file :\n",
    "    row, num_lines = 0, 10 \n",
    "    for line in file.readlines():\n",
    "        print(line.strip())\n",
    "        row += 1\n",
    "        if row == num_lines:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel ##anthe\n",
      "black ##er\n",
      "ad ##ject\n",
      "v ##ang\n",
      "betroth ##al\n",
      "tiptoe ##ing\n",
      "restroom ##s\n",
      "consol ##ing\n",
      "esp ##ionage\n",
      "influ ##x\n"
     ]
    }
   ],
   "source": [
    "with open('models/hopper-merges.txt', 'r') as file :\n",
    "    row, num_lines = 0, 10 \n",
    "    for line in reversed(file.readlines()):\n",
    "        print(line.strip())\n",
    "        row += 1\n",
    "        if row == num_lines:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num merges are slightly less than vocab size as merges don't include single character tokens, letters, numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merges: 31871\n",
      "vocab size : 32000\n"
     ]
    }
   ],
   "source": [
    "with open('models/hopper-merges.txt', 'r') as file :\n",
    "    lines = file.readlines()\n",
    "\n",
    "print(f'Number of merges: {len(lines)}')\n",
    "print(f'vocab size : {tokenizer.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61faeef881ff4a29822c46ae8f44972a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='pair', max=31870), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea2d5cb4850412589c9f3dd2f7aa53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=130, description='id', max=31999), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_pairs(pair): \n",
    "    word, token_id = vocab_sorted[pair]\n",
    "    print(f\"Word: {word}, Token ID: {token_id}\")\n",
    "\n",
    "def display_token_id(id): \n",
    "    token = tokenizer.decode([id])\n",
    "    print(f\"Token ID: {id}, Token: {token}\")\n",
    "\n",
    "_ = interact(get_pairs, pair=IntSlider(min=0, max=len(lines)-1, step=1, value=1, layout=Layout(wifth='900px')))\n",
    "_ = interact(display_token_id, id=IntSlider(min=0, max=31999, step=1, value=130, layout=Layout(wifth='900px')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: usually , he would be tearing around the living room , playing with his toys .\n",
      "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "sample = ds[0]['text']\n",
    "print(f'sample: {sample}')\n",
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >usually</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >he</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >would</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >be</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tearing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >around</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >living</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >room</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >playing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >his</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >toys</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_ids = encoding.ids\n",
    "tokens = encoding.tokens\n",
    "type_ids = encoding.type_ids\n",
    "attention_mask = encoding.attention_mask\n",
    "\n",
    "visualizer = EncodingVisualizer(tokenizer = tokenizer)\n",
    "visualizer(text=sample) # everytihng is a token in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ids</th>\n",
       "      <th>type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually</td>\n",
       "      <td>2462</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tearing</td>\n",
       "      <td>6456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>around</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>living</td>\n",
       "      <td>1559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>room</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>playing</td>\n",
       "      <td>2301</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>with</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>his</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>toys</td>\n",
       "      <td>9774</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens   ids  type_ids  attention_mask\n",
       "0   usually  2462         0               1\n",
       "1         ,    19         0               1\n",
       "2        he   149         0               1\n",
       "3     would   277         0               1\n",
       "4        be   162         0               1\n",
       "5   tearing  6456         0               1\n",
       "6    around   422         0               1\n",
       "7       the   131         0               1\n",
       "8    living  1559         0               1\n",
       "9      room   536         0               1\n",
       "10        ,    19         0               1\n",
       "11  playing  2301         0               1\n",
       "12     with   201         0               1\n",
       "13      his   177         0               1\n",
       "14     toys  9774         0               1\n",
       "15        .    21         0               1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict = {'tokens': tokens, 'ids': token_ids, 'type_ids': type_ids, 'attention_mask': attention_mask}\n",
    "df = pd.DataFrame.from_dict(out_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when encoding a batch of samples we need to pad shorter sequences in the batch, a process known as  `dynamic batching`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "samples = ds[0:4]['text']\n",
    "\n",
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "pprint(batch_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(\n",
    "    direcction = 'right', pad_id = 0, pad_token = '[PAD]', pad_type_id=0, \n",
    "    length = None, # None defaults to the max length in the batch \n",
    "    pad_to_multiple_of=None\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
      " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "batch_encoding = tokenizer.encode_batch(samples)\n",
    "pprint(batch_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
